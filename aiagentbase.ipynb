{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c10260-6176-4aa0-8915-d4676cf899f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a952b6-ba67-4958-a669-ed86af67e676",
   "metadata": {},
   "source": [
    "### Base classes for an **AI Agent** having the following architecture:\n",
    "1.1 An **AI Agent** operates in a '**World**' that calls it via '**action<-act(state)**' requests and '**reward(reward)**' intimations. (Worlds are typically wrappers around traditional RL-environments; Worlds can also wrap supervised learning tasks.) The goal of the AI Agent is to maximise *long-term steady-state average reward*. Periodically the World may also update the Agent regarding the completion of an *episode* (e.g. an RL episode or completion of epoch).\n",
    "\n",
    "1.2 An Agent has **Controller**, **Perception**, **Memory** and **Actor** components (In line with Lecun's \"Archicture of an Autonomous AI Agent\". World Model to be added later.) The Memory contains a Perceptual Memory as well as a State-Action-Reward memory. The Actor includes a **Model**. A Model includes a **Network** and **Trainer** (class that handles publishes training procedure(s) for the Network). Overall orchestration of all components including the Agent's public interface is handled by the Controller. Further, the learning schedule, to train the Network, be it done only intitally, periodically, or continually online, is decided by the Controller. \n",
    "\n",
    "1.3 Each component of an Agent may be customised for a specific World by inheriting from the default base class for that component. Agent's *reset* method resets required components and the time counter; the *clear* method clears (e.g. removes all storage) from applicable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4698e36b-02a9-4850-99d5-ecf757b4558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIAgent():\n",
    "    def __init__(self,controller=None,perception=None,memory=None,actor=None):\n",
    "        if controller is not None: self.controller=controller\n",
    "        else: self.controller=Controller()\n",
    "        if perception is not None: self.perception=perception\n",
    "        else: self.perception=Perception()\n",
    "        if memory is not None: self.memory=memory\n",
    "        else: self.memory=Memory()\n",
    "        if actor is not None: self.actor=actor\n",
    "        else: self.actor=Actor()\n",
    "        self.reset()\n",
    "        self.controller.parent=self\n",
    "        self.perception.parent=self\n",
    "        self.memory.parent=self\n",
    "        self.actor.parent=self\n",
    "    def act(self,world_state):\n",
    "        world_action = self.controller.act(world_state)\n",
    "        # check to see if network needs training - TBDesigned\n",
    "        self.time+=1\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        return self.controller.reward(world_reward)\n",
    "    def episode(self):\n",
    "        return self.controller.episode()\n",
    "    def reset(self):\n",
    "        ## TBD may need to do more\n",
    "        self.time=0\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf59ae-df32-4555-b916-0df080f8a5a7",
   "metadata": {},
   "source": [
    "2.1 Control flow goes as follows: Agent tracks a *time* counter. World calls *act*(world-state) on Agent, which is routed to Controller's *act*. The incoming *world-state* is mapped to a percept using the *perceive_state* function published by the Perception module, and stored in the perceptual memory (via Memory's *add_percept*, against the current *time* and *key*, the latter extracted from *world-state*, e.g., a ticker, ticker-date, task-id, or; default *key*='default'). ~~Note: the incoming world-state is stored as the current time's current-percept as well as the next-percept for the previous time step, for (each) incoming key.~~ \n",
    "\n",
    "2.2 The Actor reads from the perceptual memory and creates an actor-state by processing the current percept (and possibly also using prior rewards and actions, e.g. for meta-RL). The Actor also updates the state-action-reward memory with the previous time's percept after mapping it to an actor-state.\n",
    "\n",
    "2.2.1 Note: in case multiple actions are required at a given time the Actor is subclassed to override *act* so as to call the Controller's *act* multiple times and return the set of resulting set of actions together. (This will be needed in the case of tradeserver).\n",
    "\n",
    "2.3 The Actor calls its Network to decide the action to return. Before returning the action, it is stored in the perceptual memory; a new entry is also created in the state-action-reward memory with the current actor-state and action. Also, the action is mapped to a world_action using the Perception component's *action_to_world* function.\n",
    "\n",
    "2.4 Before completing the *act* (or *reward*) flow, Actor checks to see if any periodic or online training is needed to Network. It also updates the *time* counter.\n",
    "\n",
    "2.5 On receiving a *world_reward* from the World, the Actor passes it to the Controller that extracts a *key* and *reward* using Perception's *perceive_reward* function. These are stored in the perceptual memory (for the prior time step, since by now the Actor's time step has been update as soon as its action was completed) as well as appended to the latest entry (prev time step) of the state-action-reward memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "209cb81d-65f1-470d-bf88-72cfe43af453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "    def act(self,world_state):\n",
    "        percept,key=self.parent.perception.perceive_state(world_state)\n",
    "        print(percept)\n",
    "        self.parent.memory.add_percept(percept,key,self.parent.time)\n",
    "        print(self.parent.memory.perceptual_memory)\n",
    "        actor_state=self.parent.actor.create_actor_state(self.parent.time,key)\n",
    "        print(actor_state)\n",
    "        self.parent.memory.update_next_state(actor_state,self.parent.time-1)\n",
    "        print(self.parent.memory.sar_memory)\n",
    "        action=self.parent.actor.call_model(actor_state)\n",
    "        print(action)\n",
    "        self.parent.memory.add_state_action(actor_state,action,self.parent.time)\n",
    "        print(self.parent.memory.sar_memory)\n",
    "        world_action=self.parent.perception.action_to_world(action)\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        reward,key=self.parent.perception.perceive_reward(world_reward)\n",
    "        self.parent.memory.update_reward_perceptual(reward,self.parent.time-1)\n",
    "        self.parent.memory.update_reward_sar(reward,self.parent.time-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29715c63-7fc0-4941-9fa5-30cdaf02543b",
   "metadata": {},
   "source": [
    "3.1 ***Memory*** stores are nested dictionaries indexed by *time* and *key*. Each entry is a dictionary with keys *'percept','action','reward'* and *'state','action','reward','next_state'* for perceptual memory / state-action-reward memory respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "28661473-fc0b-470c-93cd-2cc08c45c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "        self.clear()\n",
    "    def clear(self):\n",
    "        self.perceptual_memory={}\n",
    "        self.sar_memory={}\n",
    "    def add_percept(self,percept,key,time):\n",
    "        if time in self.perceptual_memory: self.perceptual_memory[time][key]={'percept':percept}\n",
    "        else: self.perceptual_memory[time]={key:{'percept':percept}}\n",
    "    def update_next_state(self,actor_state,time):\n",
    "        if time in self.sar_memory: self.sar_memory[time]['next_state']=actor_state\n",
    "        else: self.sar_memory[time]={'next_state':actor_state}\n",
    "    def add_state_action(self,actor_state,action,time):\n",
    "        if time in self.sar_memory: \n",
    "            self.sar_memory[time]['state']=actor_state\n",
    "            self.sar_memory[time]['action']=action\n",
    "        else: self.sar_memory[time]={'state':actor_state,'action':action}\n",
    "    def update_reward_perceptual(self,reward,time):\n",
    "        self.perceptual_memory[time]['reward']=reward\n",
    "    def update_reward_sar(self,reward,time):\n",
    "        self.sar_memory[time]['reward']=reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba530e0a-0173-4319-a133-0616b0d08eae",
   "metadata": {},
   "source": [
    "3.2 The default **Perception** class just copies world states/actions/rewards to actor states/actions/rewards. Should be subclassed for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e870706-16c2-491c-9fbd-fefd9bd3a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perception():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "    def perceive_state(self,world_state):\n",
    "        return world_state,'default'\n",
    "    def action_to_world(self,action):\n",
    "        return action\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward,'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e3ecd-7499-4bdb-b268-4c3a9264a57c",
   "metadata": {},
   "source": [
    "3.3 The default **Actor** has no Model and returns a fixed action (can be set). It copies the percept from perceptual memory directly into the actor_state. This should be subclassed and/or method *percept_to_state* or *create_actor_state* overridden for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "490d7d18-d6ab-4a72-b83e-d73b4c541596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self,model=None):\n",
    "        self.parent=None\n",
    "        if model is not None: self.model=model\n",
    "        self.default_action='default_action'\n",
    "    def create_actor_state(self,time,key):\n",
    "        if time not in self.parent.memory.perceptual_memory: return None\n",
    "        elif key not in self.parent.memory.perceptual_memory[time]: return None\n",
    "        elif 'percept' not in self.parent.memory.perceptual_memory[time][key]: return None\n",
    "        else: return self.percept_to_state(self.parent.memory.\n",
    "                                           perceptual_memory[time][key]['percept'])\n",
    "    def percept_to_state(self,percept):\n",
    "        return percept\n",
    "    def call_model(self,actor_state):\n",
    "        return self.default_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6cdce3cc-5e27-4b21-a775-23c4eb683b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=AIAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "53b84509-bd7f-4344-91ef-4ecf54a1c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world_state_2\n",
      "{0: {'default': {'percept': 'world_state_1'}, 'reward': 100}, 1: {'default': {'percept': 'world_state_2'}}}\n",
      "world_state_2\n",
      "{-1: {'next_state': 'world_state_1'}, 0: {'state': 'world_state_1', 'action': 'default_action', 'reward': 100, 'next_state': 'world_state_2'}}\n",
      "default_action\n",
      "{-1: {'next_state': 'world_state_1'}, 0: {'state': 'world_state_1', 'action': 'default_action', 'reward': 100, 'next_state': 'world_state_2'}, 1: {'state': 'world_state_2', 'action': 'default_action'}}\n"
     ]
    }
   ],
   "source": [
    "agent.act('world_state_2')\n",
    "agent.reward(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6bcb9812-6ea6-4632-8f1a-d0957dcf26dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.perceptual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "364782d3-e034-49cc-a29d-cdd38c385ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.sar_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09642a27-9466-477d-b3ac-bba17a647746",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reward(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "014eaf32-3610-4c03-9126-48543ed6a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ce33ad2-c635-484a-8041-4faac6a0f424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d9471-699d-4de0-b9a5-cd7af17ce327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
