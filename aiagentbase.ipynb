{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c10260-6176-4aa0-8915-d4676cf899f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a952b6-ba67-4958-a669-ed86af67e676",
   "metadata": {},
   "source": [
    "### Base classes for an **AI Agent** having the following architecture:\n",
    "1.1 An **AI Agent** operates in a '**World**' that calls it via '**action<-act(state)**' requests and '**reward(reward)**' intimations. (Worlds are typically wrappers around traditional RL-environments; Worlds can also wrap supervised learning tasks.) The goal of the AI Agent is to maximise *long-term steady-state average reward*. Periodically the World may also update the Agent regarding the completion of an *episode* (e.g. an RL episode or completion of epoch).\n",
    "\n",
    "1.2 An Agent has **Controller**, **Perception**, **Memory** and **Actor** components (In line with Lecun's \"Archicture of an Autonomous AI Agent\". World Model to be added later.) The Memory contains a Perceptual Memory as well as a State-Action-Reward memory. The Actor includes a **Model**. A Model includes a **Network** and **Trainer** (class that handles publishes training procedure(s) for the Network). Overall orchestration of all components including the Agent's public interface is handled by the Controller. Further, the learning schedule, to train the Network, be it done only intitally, periodically, or continually online, is decided by the Controller. \n",
    "\n",
    "1.3 Each component of an Agent may be customised for a specific World by inheriting from the default base class for that component. Agent's *begin* method indicates that a new episode/epoch is starting and resets/increments the time/ep counters (see below); the *clear* method clears (e.g. removes all storage) from applicable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4698e36b-02a9-4850-99d5-ecf757b4558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIAgent():\n",
    "    def __init__(self,controller=None,perception=None,memory=None,actor=None):\n",
    "        if controller is not None: self.controller=controller\n",
    "        else: self.controller=Controller()\n",
    "        if perception is not None: self.perception=perception\n",
    "        else: self.perception=Perception()\n",
    "        if memory is not None: self.memory=memory\n",
    "        else: self.memory=Memory()\n",
    "        if actor is not None: self.actor=actor\n",
    "        else: self.actor=Actor()\n",
    "        self.time=0\n",
    "        self.ep=[]\n",
    "        self.controller.parent=self\n",
    "        self.perception.parent=self\n",
    "        self.memory.parent=self\n",
    "        self.actor.parent=self\n",
    "        self.debug=False\n",
    "        self.use_memory=True\n",
    "    def act(self,world_state):\n",
    "        world_action = self.controller.act(world_state)\n",
    "        # check to see if network needs training - TBDesigned\n",
    "        self.time+=1\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        return self.controller.reward(world_reward)\n",
    "    def episode(self):\n",
    "        return self.controller.episode()\n",
    "    def begin(self,state=None):\n",
    "        ## TBD may need to do more - both episode and time may be needed and episode reset\n",
    "        self.ep+=[self.time]\n",
    "    def clear(self):\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf59ae-df32-4555-b916-0df080f8a5a7",
   "metadata": {},
   "source": [
    "2.1 Control flow goes as follows: Agent tracks a *time* counter and a list *ep*; the latter tracks episodes/epochs and is set to the first time counter for each epoch/episode e.g. ep[e]=starting time of epoch/epsode e. World calls *act*(world-state) on Agent, which is routed to Controller's *act*. The incoming *world-state* is mapped to a percept using the *perceive_state* function published by the Perception module, and stored in the perceptual memory (via Memory's *add_percept*, against the current *time* and *key*, the latter extracted from *world-state*, e.g., a task-id, or; default *key*='default'). \n",
    "\n",
    "2.2 The Actor reads from the perceptual memory and creates an actor-state by processing the current percept (and possibly also using prior rewards and actions, e.g. for meta-RL). The Actor also updates the state-action-reward memory with the previous time's percept after mapping it to an actor-state.\n",
    "\n",
    "2.3 The Actor calls its Model to decide the action to return. Before returning the action, it is stored in the perceptual memory; a new entry is also created in the state-action-reward memory with the current actor-state and action. Also, the action is mapped to a world_action using the Perception component's *action_to_world* function.\n",
    "\n",
    "2.4 Before completing the *act* (or *reward*) flow, Actor checks to see if any periodic or online training is needed, and updates the *training* flag accordingly. It also updates the *time* counter.\n",
    "\n",
    "2.5 On receiving a *world_reward* from the World, the Actor passes it to the Controller that extracts a *key*, *reward* and *info* using Perception's *perceive_reward* function.  These are stored in the perceptual memory (for the prior time step, since by now the Actor's time step has been update as soon as its action was completed) as well as appended to the latest entry (prev time step) of the state-action-reward memory. Note: *info* might include e.g. labels in case of a supervised learning scenario, or may be empty e.g. in black-box learning (RL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209cb81d-65f1-470d-bf88-72cfe43af453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "    def act(self,world_state):\n",
    "        percept,key,info=self.parent.perception.perceive_state(world_state)\n",
    "        if self.parent.debug: print('percept:',percept,'key:',key)\n",
    "        if self.parent.use_memory: self.parent.memory.add_percept(percept,info,key,self.parent.time)\n",
    "        if self.parent.use_memory: actor_state=self.parent.actor.create_actor_state(self.parent.time,key)\n",
    "        else: actor_state=self.parent.actor.percept_to_state(percept)\n",
    "        if self.parent.debug: print('actor_state:',actor_state)\n",
    "        if self.parent.use_memory: self.parent.memory.update_next_state(actor_state,self.parent.time-1)\n",
    "        action=self.parent.actor.call_model(actor_state)\n",
    "        if self.parent.debug: print('action:',action)\n",
    "        if self.parent.use_memory: self.parent.memory.update_action_perceptual(action,key,self.parent.time)\n",
    "        if self.parent.use_memory: self.parent.memory.add_state_action(actor_state,action,self.parent.time)\n",
    "        world_action=self.parent.perception.action_to_world(action)\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        reward,key,info=self.parent.perception.perceive_reward(world_reward)\n",
    "        if self.parent.use_memory: self.parent.memory.update_reward_perceptual(reward,self.parent.time-1,key,info)\n",
    "        if self.parent.use_memory: self.parent.memory.update_reward_sar(reward,self.parent.time-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29715c63-7fc0-4941-9fa5-30cdaf02543b",
   "metadata": {},
   "source": [
    "3.1 ***Memory*** stores are nested dictionaries indexed by *time* and *key*. Each entry is a dictionary with keys *'percept','action','reward'* and *'state','action','reward','next_state'* for perceptual memory / state-action-reward memory respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28661473-fc0b-470c-93cd-2cc08c45c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "        self.clear()\n",
    "    def clear(self):\n",
    "        self.perceptual_memory={}\n",
    "        self.sar_memory={}\n",
    "    def add_percept(self,percept,info,key,time):\n",
    "        if time in self.perceptual_memory: \n",
    "            self.perceptual_memory[time][key]={'percept':percept,'info':info}\n",
    "        else: self.perceptual_memory[time]={key:{'percept':percept,'info':info}}\n",
    "        if self.parent.debug: print('add_percept:',self.perceptual_memory)\n",
    "    def update_next_state(self,actor_state,time):\n",
    "        if time in self.sar_memory: self.sar_memory[time]['next_state']=actor_state\n",
    "        else: self.sar_memory[time]={'next_state':actor_state}\n",
    "        if self.parent.debug: print('update_next_state:',self.sar_memory)\n",
    "    def update_action_perceptual(self,action,key,time):\n",
    "        self.perceptual_memory[time][key]['action']=action\n",
    "        if self.parent.debug: print('update_action_perceptual:',self.perceptual_memory)\n",
    "    def add_state_action(self,actor_state,action,time):\n",
    "        if time in self.sar_memory: \n",
    "            self.sar_memory[time]['state']=actor_state\n",
    "            self.sar_memory[time]['action']=action\n",
    "        else: self.sar_memory[time]={'state':actor_state,'action':action}\n",
    "        if self.parent.debug: print('add_state_action:',self.sar_memory)\n",
    "    def update_reward_perceptual(self,reward,time,key='default',info={}):\n",
    "        self.perceptual_memory[time][key]['reward']=reward\n",
    "        self.perceptual_memory[time][key]['info']=info\n",
    "        if self.parent.debug: print('update_reward_perceptual:',self.perceptual_memory)\n",
    "    def update_reward_sar(self,reward,time):\n",
    "        self.sar_memory[time]['reward']=reward\n",
    "        if self.parent.debug: print('update_reward_sar:',self.sar_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba530e0a-0173-4319-a133-0616b0d08eae",
   "metadata": {},
   "source": [
    "3.2 The default **Perception** class just copies world states/actions/rewards to actor states/actions/rewards. Should be subclassed for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e870706-16c2-491c-9fbd-fefd9bd3a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perception():\n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "    def perceive_state(self,world_state):\n",
    "        return world_state,'default',{}\n",
    "    def action_to_world(self,action):\n",
    "        return action\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward,'default',{}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e3ecd-7499-4bdb-b268-4c3a9264a57c",
   "metadata": {},
   "source": [
    "3.3 The default **Actor** has no Model and returns a fixed action (can be set). It copies the percept from perceptual memory directly into the actor_state. This should be subclassed and/or method *percept_to_state* or *create_actor_state* overridden for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490d7d18-d6ab-4a72-b83e-d73b4c541596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self,model=None):\n",
    "        self.parent=None\n",
    "        if model is not None: self.model=model\n",
    "        self.default_action='default_action'\n",
    "    def create_actor_state(self,time,key):\n",
    "        if time not in self.parent.memory.perceptual_memory: return None\n",
    "        elif key not in self.parent.memory.perceptual_memory[time]: return None\n",
    "        elif 'percept' not in self.parent.memory.perceptual_memory[time][key]: return None\n",
    "        else: return self.percept_to_state(self.parent.memory.\n",
    "                                           perceptual_memory[time][key]['percept'])\n",
    "    def percept_to_state(self,percept):\n",
    "        return percept\n",
    "    def call_model(self,actor_state):\n",
    "        return self.default_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2066b-e0f1-42ca-a7fb-3301fa4e72d2",
   "metadata": {},
   "source": [
    "### Guidelines on overriding/augmenting base classes for world-specific agents\n",
    "Template indicating methods that need to be overridden/augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf26c60-2c88-43f3-bf05-ee74bbc3017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateAIAgent(AIAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##Augmenting AIAgent\n",
    "        self.actor.call_model=self.call_model\n",
    "        self.perception.perceive_state=self.perceive_state\n",
    "        self.perception.perceive_reward=self.perceive_reward\n",
    "        self.perception.action_to_world=self.action_to_world\n",
    "        self.actor.percept_to_state=self.percept_to_state\n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        return super().reward(reward)\n",
    "    def begin(self,state):\n",
    "        ##Augmenting AIAgent\n",
    "        super().begin(state)\n",
    "    def call_model(self,state):\n",
    "        ##Overriding AIAgent\n",
    "        action=self.action_space.sample() #override with actual policy\n",
    "        return action\n",
    "    def perceive_state(self,world_state):\n",
    "        #Override AIAgent\n",
    "        percept=world_state\n",
    "        return percept\n",
    "    def percept_to_state(self,percept):\n",
    "        #Override AIAgent\n",
    "        actor_state=percept\n",
    "        return actor_state\n",
    "    def perceive_reward(self,reward):\n",
    "        #Override AIAgent\n",
    "        actor_reward=reward\n",
    "        return actor_reward\n",
    "    def action_to_world(self,action):\n",
    "        ##Override AIAgent\n",
    "        world_action=action\n",
    "        return world_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
