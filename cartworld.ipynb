{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8289f-e2f8-43d4-bfd4-b0913edf0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd282224-20ee-4bb0-85e8-82a65cb6538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread in threading.enumerate(): \n",
    "    print(thread.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55b243-bf59-4ac8-93c8-e001b25737b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for thread in threading.enumerate(): \n",
    "#     print(thread.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0c3d3-6686-4413-b5be-f9bd31ad73ab",
   "metadata": {},
   "source": [
    "### Agent-based RL in Simple Worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175c769-e996-4efc-a2e3-e13cfb50c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6517152-bd6d-4d36-a5b6-a8b95f2d100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f4fa-0689-4463-9fdf-48908932a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartWorld():\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.test_episodes=[]\n",
    "        self.world_over=False\n",
    "    def stop(self):\n",
    "        self.world_over=True\n",
    "    def run(self,agent=None,n_episodes=10,episode_maxlen=10):\n",
    "        agent.observation_space=env.observation_space\n",
    "        if 'training' not in agent.__dict__: agent.training=False\n",
    "        if agent.training: testing=False \n",
    "        else: testing=True\n",
    "        if agent.training: print('Starting Training time: ',agent.time)\n",
    "        for episode in range(n_episodes):\n",
    "            # print('CartAgent','starting episode')\n",
    "            state=self.env.reset()\n",
    "            agent.begin()\n",
    "            # print(agent.time)#,agent.ep)\n",
    "            for t in range(episode_maxlen):\n",
    "                # env.render(mode='rgb_array')\n",
    "                action=agent.act(state)\n",
    "                # print(episode,t,'Action: ', action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                agent.reward((reward,done,info))\n",
    "                # print(episode,t,'Reward sent: ', reward)\n",
    "                if done:\n",
    "                    break\n",
    "            if self.world_over:break\n",
    "            if not agent.training: self.test_episodes+=[episode]\n",
    "            if not agent.training and not testing: \n",
    "                print('Training Over at time: ',agent.time)\n",
    "                testing=True\n",
    "        print('Testing Done time: ', agent.time, ' Reward: ', agent.avg_rew())\n",
    "        return agent.avg_rew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec537ae8-fbf6-4c49-b16c-031b37228142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesnt use AIAgent Architecture Classes but implements the same interface - for initial testing\n",
    "class RandomAgent():\n",
    "    def __init__(self,action_space):\n",
    "        self.action_space=action_space\n",
    "        self.tot_rew=0\n",
    "        self.rewL=[]\n",
    "    def act(self,state):\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    def reward(self,rew):\n",
    "        self.tot_rew+=rew[0]\n",
    "    def begin(self,state):\n",
    "        self.rewL+=[self.tot_rew]\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95733e-f863-432b-ac20-753d221e6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAIAgent(AIAgent):\n",
    "    def __init__(self,action_space):\n",
    "        super().__init__()\n",
    "        self.actor=self.Actor(parent=self)\n",
    "        self.action_space=action_space\n",
    "        self.tot_rew=0\n",
    "        self.rewL=[]\n",
    "        \n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent): \n",
    "            self.parent=parent\n",
    "        def call_model(self,state):\n",
    "        ##Overriding AIAgent.Model\n",
    "            action = self.parent.action_space.sample()\n",
    "            return action\n",
    "    \n",
    "    def reward(self,rew):\n",
    "        ##Augmenting AIAgent\n",
    "        self.tot_rew+=rew[0]\n",
    "        return super().reward(rew)\n",
    "    def begin(self):\n",
    "        ##Augmenting AIAgent\n",
    "        self.rewL+=[self.tot_rew]\n",
    "        super().begin()\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01edb66-b700-4f8b-a922-069b1de00c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=RandomAIAgent(env.action_space)\n",
    "agent.training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712783c-fd5a-420a-9b27-91f2c5a28e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.debug=False\n",
    "agent.use_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d0351-b72d-481f-a3ea-f100ca10a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=CartWorld(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0adf9-f667-49a4-a141-a60d9d595ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldthread=Thread(name='world',target=world.run,args=(agent,1000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854c490-8ed2-4f50-9e46-8627525e843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldthread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8644cbd-6139-491d-8f12-720a4b619e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# world.run(agent,10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e24475-66fc-4544-a0fe-20297b9246ac",
   "metadata": {},
   "source": [
    "### Training an AI Agent's Model using an off-the shelf RL procedure\n",
    "Create a local environment called by a Monitor thread running within Agent, which implements or\n",
    "re-uses an on-policy RL training procedure (such as PPO etc.). The env has an input and output queue. If a training flag is set, the Agent uses a ProxyModel place of its normal model to compute actions: The actor-state is placed in the env's input queue and an action is awaited from the env's output queue.\n",
    "\n",
    "The monitor starts by calling the env.reset method that waits on the input queue to receive \n",
    "and then return a state. The monitor thread computes an action on the current state and calls\n",
    "env.step(action), which places the action in the output queue and awaits a reward from the input\n",
    "queue. After receiving a reward, step again waits for the next state on the input queue.\n",
    "Once this is also received, both next stte and reward are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefe229-241d-42fb-b5a2-0fe4b4f36c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import threading\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819bbb0-dc30-46d0-9c8e-3c5cf11e74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(AIAgent):\n",
    "    def __init__(self,action_space,observation_space,training_steps=20000):\n",
    "        ##Augmenting AIAgent\n",
    "        super().__init__()\n",
    "        ##Local RL environment running in a thread to interact with World via queues\n",
    "        self.env=self.TrainingEnv(parent=self)\n",
    "        self.env.action_space=action_space\n",
    "        self.env.observation_space=observation_space\n",
    "        self.monitor=self.Monitor(parent=self)\n",
    "        self.set_training(True)\n",
    "        self.monitorthread=Thread(name='monitor',target=self.monitor.train,args=(training_steps,))\n",
    "        self.tot_rew=0\n",
    "        self.logL=[]\n",
    "        self.kill=False\n",
    "        ##Override Actor\n",
    "        model=PPO('MlpPolicy', self.env, verbose=0)\n",
    "        self.actor=self.Actor(parent=self,model=model)\n",
    "        \n",
    "    def start(self):\n",
    "        self.monitorthread.start() \n",
    "    \n",
    "    def stop(self):\n",
    "        self.kill=True\n",
    "    \n",
    "    def log(self,entry):\n",
    "        self.logL+=[entry]\n",
    "        \n",
    "    def set_training(self,value):\n",
    "        self.training=value\n",
    "        \n",
    "    class TrainingEnv(gym.Env):\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "            # self.observation_space=spaces.Box(\n",
    "            #     low=np.array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38]), \n",
    "            #     high=np.array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38]), \n",
    "            #     shape=(4,), dtype=np.float32)\n",
    "            self.inputS=Queue() #written by act read by reset and step\n",
    "            self.outputS=Queue() #written by act read by act\n",
    "            self.rewardI=Queue() #written by act and read by step\n",
    "            self.actionO=Queue() #written by step and read by act\n",
    "            self.counter=0\n",
    "        def reset(self):\n",
    "            # print('reset')\n",
    "            self.parent.log(('reset waiting',self.counter))\n",
    "            self.state=self.inputS.get()\n",
    "            return self.state\n",
    "        def step(self,action):\n",
    "            if self.parent.kill: sys.exit(-1)\n",
    "            # print('step')\n",
    "            self.actionO.put(action)\n",
    "            self.parent.log(('step put action',self.counter))\n",
    "            self.parent.log((self.counter,action))\n",
    "            self.parent.log(('step waiting for reward',self.counter))\n",
    "            reward,done,info=self.rewardI.get()\n",
    "            if not done:\n",
    "                self.parent.log(('step waiting for next state',self.counter))\n",
    "                next_state=self.inputS.get()\n",
    "                self.counter+=1\n",
    "                self.parent.log((self.counter,next_state))\n",
    "            else: next_state=self.state\n",
    "            # print(self.counter,done)\n",
    "            return next_state,reward,done,info\n",
    "        def print_queues(self):\n",
    "            print('inputS',self.inputS.queue)\n",
    "            print('actionO',self.actionO.queue)\n",
    "            print('rewardI',self.rewardI.queue)\n",
    "            print('outputS',self.outputS.queue)\n",
    "             \n",
    "    class Monitor():\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "        def run(self,training_steps,N_EPISODES=10):\n",
    "            for episode in range(N_EPISODES):\n",
    "                state=self.parent.env.reset()\n",
    "                for steps in range(training_steps):\n",
    "                    # env.render()\n",
    "                    action=self.parent.env.action_space.sample()\n",
    "                    state, reward, done, info = self.parent.env.step(action)\n",
    "                    # print(next_state, reward, done, info, action)\n",
    "            self.parent.set_training(False)\n",
    "        def train(self,training_steps):\n",
    "            self.parent.actor.model.learn(total_timesteps=training_steps)\n",
    "            self.parent.env.actionO.put(self.parent.env.action_space.sample())\n",
    "            self.parent.set_training(False)\n",
    "            # self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent,model):\n",
    "            super().__init__(parent=parent,model=model)\n",
    "        def call_model(self,state):\n",
    "        ##Overriding AIAgent\n",
    "            time=self.parent.time\n",
    "            if self.parent.training: \n",
    "                self.parent.env.inputS.put(state)\n",
    "                self.parent.log(('call model put state at time',time))\n",
    "                self.parent.log(('call model waiting for action at time',time))\n",
    "                try: action = self.parent.env.actionO.get()#timeout=5)\n",
    "                except: action=0\n",
    "                self.parent.log(('call model received action at time',time))\n",
    "            else: action, _states = self.model.predict(state)\n",
    "            return action\n",
    "    \n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        reward=super().reward(reward)\n",
    "        self.tot_rew+=reward[0]\n",
    "        if self.training: \n",
    "            self.env.rewardI.put(reward)\n",
    "            self.log(('call model put reward at time',self.time))\n",
    "    def begin(self):\n",
    "        ##Augmenting AIAgent\n",
    "        self.rewL+=[self.tot_rew]\n",
    "        super().begin()\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f938dfc-36bd-4e18-87c0-584c3c8c0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=RLAgent(action_space=env.action_space,observation_space=env.observation_space,training_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edef18-00b7-4098-96f8-01f51bccc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.debug=False\n",
    "agent.use_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cce67f-0995-42fa-b1a1-1753133a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rewL=[]\n",
    "agent.tot_rew=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd4736-40ee-4be8-9d65-95804512b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7d8e-eae8-43f4-98b2-28f652813c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=CartWorld(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ee6db-c404-4453-a405-c2cf8c54804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldthread=Thread(name='world',target=world.run,args=(agent,2000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295f9e2-c817-46d1-9078-c2135216f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldthread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2eb86f-9e74-44e8-b478-6d0ce518b196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# world.run(agent,n_episodes=2000,episode_maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd2621-fe0a-4189-9dd4-436dba7f2301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(agent.ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9f929-b8df-4a24-9f21-cbb91cb14a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4a528-95bd-46df-89b5-24208287a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44ee84-a7ee-4c78-a177-21f84f5243bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_len=len([agent.rewL[t] for t in world.test_episodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554e4c8-2681-4bd6-b805-b708fa863ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24174f7-ddec-495e-bd52-9612889571c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.gradient(agent.rewL[testing_len:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b52fb-577e-4ebd-9b8d-d63b3c1680fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(agent.rewL[testing_len:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b82967-748a-4909-9228-8d2de329388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "rewL=[]\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    steps=0\n",
    "    while not done and steps<=200:\n",
    "        # env.render()\n",
    "        action,_ = agent.model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        steps+=1\n",
    "    # print('Episode:{} Score:{}'.format(episode, score))\n",
    "    rewL+=[score]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee784d-2f75-4e2f-8f8c-91ae3b607cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e78bfe-2dda-4fe5-a39d-06c88022a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rewL).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26254efc-9de2-479c-95ec-d9a68f5ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b11bb-5c03-4c58-9d83-29343655ddba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
