{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e8289f-e2f8-43d4-bfd4-b0913edf0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c175c769-e996-4efc-a2e3-e13cfb50c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6517152-bd6d-4d36-a5b6-a8b95f2d100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from aiagentbase.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e9f4fa-0689-4463-9fdf-48908932a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartWorld():\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "    def run(self,agent=None,n_episodes=30,episode_maxlen=300):\n",
    "        global world_over\n",
    "        agent.observation_space=env.observation_space\n",
    "        if 'training' not in agent.__dict__: agent.training=False\n",
    "        world_over=False\n",
    "        for episode in range(n_episodes):\n",
    "            # print('CartAgent','starting episode')\n",
    "            if world_over: break\n",
    "            state=env.reset()\n",
    "            agent.begin(state)\n",
    "            # print(agent.time,agent.ep)\n",
    "            for t in range(episode_maxlen):\n",
    "                if world_over: break\n",
    "                # env.render(mode='rgb_array')\n",
    "                action=agent.act(env.state)\n",
    "                # print('TrainingEnv queues:',agent.env.print_queues())\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                agent.reward((reward,done,info))\n",
    "                # print('Monitor over:',agent.monitor.over,'TrainingEnv counter:',\n",
    "                #       agent.env.counter)\n",
    "                if done or world_over:\n",
    "                    break\n",
    "        return agent.avg_rew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec537ae8-fbf6-4c49-b16c-031b37228142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesnt use AIAgent Architecture Classes but implements the same interface - for initial testing\n",
    "class RandomAgent():\n",
    "    def __init__(self,action_space):\n",
    "        self.action_space=action_space\n",
    "        self.tot_rew=0\n",
    "        self.rewL=[]\n",
    "    def act(self,state):\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    def reward(self,rew):\n",
    "        self.tot_rew+=rew[0]\n",
    "    def begin(self,state):\n",
    "        self.rewL+=[self.tot_rew]\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a95733e-f863-432b-ac20-753d221e6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAIAgent(AIAgent):\n",
    "    def __init__(self,action_space):\n",
    "        super().__init__()\n",
    "        ##Augmenting AIAgent\n",
    "        self.actor.call_model=self.call_model\n",
    "        # self.actor.percept_to_state=self.percept_to_state\n",
    "        # self.perception.perceive_state=self.perceive_state\n",
    "        # self.perception.perceive_reward=self.perceive_reward\n",
    "        self.action_space=action_space\n",
    "        self.tot_rew=0\n",
    "        self.rewL=[]\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)\n",
    "    def reward(self,rew):\n",
    "        ##Augmenting AIAgent\n",
    "        self.tot_rew+=rew[0]\n",
    "        return super().reward(rew)\n",
    "    def begin(self,state):\n",
    "        ##Augmenting AIAgent\n",
    "        self.rewL+=[self.tot_rew]\n",
    "        super().begin()\n",
    "    def call_model(self,state):\n",
    "        ##Overriding AIAgent.Model\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "    # def perceive_state(self,world_state):\n",
    "    #     #Override AIAgent\n",
    "    #     percept=world_state\n",
    "    #     return percept\n",
    "    # def percept_to_state(self,percept):\n",
    "    #     #Override AIAgent\n",
    "    #     actor_state=percept\n",
    "    #     return actor_state\n",
    "    def perceive_reward(self,reward):\n",
    "        rew=reward[0]\n",
    "        info=reward[2]\n",
    "        return rew,'default',info\n",
    "    # def action_to_world(self,action):\n",
    "    #     ##Override AIAgent\n",
    "    #     return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01edb66-b700-4f8b-a922-069b1de00c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=RandomAIAgent(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9712783c-fd5a-420a-9b27-91f2c5a28e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.debug=False\n",
    "agent.use_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5d0351-b72d-481f-a3ea-f100ca10a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=CartWorld(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8644cbd-6139-491d-8f12-720a4b619e07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.run(agent,10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e24475-66fc-4544-a0fe-20297b9246ac",
   "metadata": {},
   "source": [
    "### Training an AI Agent's Model using an off-the shelf RL procedure\n",
    "Create a local environment called by a Monitor thread running within Agent, which implements or\n",
    "re-uses an on-policy RL training procedure (such as PPO etc.). The env has an input and output queue. If a training flag is set, the Agent uses a ProxyModel place of its normal model to compute actions: The actor-state is placed in the env's input queue and an action is awaited from the env's output queue.\n",
    "\n",
    "The monitor starts by calling the env.reset method that waits on the input queue to receive \n",
    "and then return a state. The monitor thread computes an action on the current state and calls\n",
    "env.step(action), which places the action in the output queue and awaits a reward from the input\n",
    "queue. After receiving a reward, step again waits for the next state on the input queue.\n",
    "Once this is also received, both next stte and reward are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefe229-241d-42fb-b5a2-0fe4b4f36c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819bbb0-dc30-46d0-9c8e-3c5cf11e74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(RandomAIAgent):\n",
    "    def __init__(self,action_space,observation_space,training_steps=20000):\n",
    "        ##Augmenting AIAgent\n",
    "        # self.model=model = PPO.load('ReinforcementLearningCourse-main/Training/Saved Models/PPO_model', env=env)\n",
    "        super().__init__(action_space)\n",
    "        self.env=self.TrainingEnv(parent=self)\n",
    "        self.env.observation_space=observation_space\n",
    "        self.model=PPO('MlpPolicy', self.env, verbose=0)\n",
    "        self.monitor=self.Monitor(parent=self)\n",
    "        # self.monitorthread=Thread(target=self.monitor.run) #For dubugging\n",
    "        self.set_training(True)\n",
    "        self.monitorthread=Thread(target=self.monitor.train,args=(training_steps,))\n",
    "        self.monitorthread.start()\n",
    "        self.tot_rew=0\n",
    "        self.logL=[]\n",
    "    \n",
    "    def log(self,entry):\n",
    "        self.logL+=[entry]\n",
    "        \n",
    "    def set_training(self,value):\n",
    "        self.training=value\n",
    "        \n",
    "    class TrainingEnv(gym.Env):\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "            self.action_space=spaces.Discrete(2)\n",
    "            # self.observation_space=spaces.Box(\n",
    "            #     low=np.array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38]), \n",
    "            #     high=np.array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38]), \n",
    "            #     shape=(4,), dtype=np.float32)\n",
    "            self.inputS=Queue() #written by act read by reset and step\n",
    "            self.outputS=Queue() #written by act read by act\n",
    "            self.rewardI=Queue() #written by act and read by step\n",
    "            self.actionO=Queue() #written by step and read by act\n",
    "            self.counter=0\n",
    "        def reset(self):\n",
    "            # print('reset')\n",
    "            state=self.inputS.get()\n",
    "            return state\n",
    "        def step(self,action):\n",
    "            # print('step')\n",
    "            self.actionO.put(action)\n",
    "            reward,done,info=self.rewardI.get()\n",
    "            next_state=self.inputS.get()\n",
    "            self.counter+=1\n",
    "            # print(self.counter,done)\n",
    "            return next_state,reward,done,info\n",
    "        def print_queues(self):\n",
    "            print('inputS',self.inputS.queue)\n",
    "            print('actionO',self.actionO.queue)\n",
    "            print('rewardI',self.rewardI.queue)\n",
    "            print('outputS',self.outputS.queue)\n",
    "             \n",
    "    class Monitor():\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "        def run(self):\n",
    "            state=env.reset()\n",
    "            for episode in range(600):\n",
    "                # env.render()\n",
    "                action=self.parent.env.action_space.sample()\n",
    "                next_state, reward, done, info = self.parent.env.step(action)\n",
    "                print(next_state, reward, done, info, action)\n",
    "            self.parent.monitorthread.join()\n",
    "        def train(self,training_steps):\n",
    "            global world_over\n",
    "            self.parent.model.learn(total_timesteps=training_steps)\n",
    "            print('Training Over')\n",
    "            self.parent.set_training(False)\n",
    "            self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "    def call_model(self,state):\n",
    "        ##Overriding AIAgent\n",
    "        if self.training:\n",
    "            self.env.inputS.put(state)\n",
    "            try: action = self.env.actionO.get(timeout=5)\n",
    "            except: action=0\n",
    "        else: action, _states = self.model.predict(state)\n",
    "        return action\n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        if self.training: self.env.rewardI.put(reward)\n",
    "        super().reward(reward)\n",
    "    def begin(self,state):\n",
    "        ##Augmenting AIAgent\n",
    "        if self.training: self.env.inputS.put(state)\n",
    "        super().begin(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f938dfc-36bd-4e18-87c0-584c3c8c0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=PPOAgent(env.action_space,env.observation_space,training_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edef18-00b7-4098-96f8-01f51bccc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.debug=False\n",
    "agent.use_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cce67f-0995-42fa-b1a1-1753133a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rewL=[]\n",
    "agent.tot_rew=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7d8e-eae8-43f4-98b2-28f652813c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=CartWorld(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de878c07-e022-4e18-96dc-52c98494bf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world.run(agent,n_episodes=2000,episode_maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4a528-95bd-46df-89b5-24208287a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24174f7-ddec-495e-bd52-9612889571c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.gradient(agent.rewL).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b52fb-577e-4ebd-9b8d-d63b3c1680fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(agent.rewL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b97138-511d-4bac-acac-b20f19e2c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread in threading.enumerate(): \n",
    "    print(thread.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828025a-aa04-4f34-9aa9-c20f9a405b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(agent.memory.sar_memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a61bf-8782-496d-8b66-fb6e1d2eb70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.memory.perceptual_memory[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f5a0d-b924-4997-b954-b35e6a77388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=agent.memory.perceptual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56252997-3220-408e-bcf0-5d02dafb5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[M[t]['default']['percept'] for t in M if 'percept' in M[t]['default']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee502a-a3c9-4cac-9dc0-42aee86aadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[M[t]['default']['percept'] for t in M][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235bdd1-4f19-4e44-b255-55b14ca636b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
