{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8289f-e2f8-43d4-bfd4-b0913edf0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c0c4f-ad65-4e65-a307-dd2db1c32ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0c3d3-6686-4413-b5be-f9bd31ad73ab",
   "metadata": {},
   "source": [
    "### World and Agents for Supervised Learning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6517152-bd6d-4d36-a5b6-a8b95f2d100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f4fa-0689-4463-9fdf-48908932a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLWorld():\n",
    "    def __init__(self,train_ds,test_ds,n_classes):\n",
    "        self.train_ds=train_ds\n",
    "        self.test_ds=test_ds\n",
    "        self.action_space=spaces.Discrete(n_classes)\n",
    "    def run(self,agent=None,n_episodes=10):\n",
    "        self.test_rew=0\n",
    "        self.test_rewL=[]\n",
    "        agent.set_training(True)\n",
    "        if 'training' not in agent.__dict__: agent.training=False\n",
    "        for episode in range(n_episodes):\n",
    "            tot_rew=0\n",
    "            agent.begin()\n",
    "            for sample,label in self.train_ds:\n",
    "                action=agent.act(sample)\n",
    "                reward=(self.accuracy(action,label),{'default'},{'label':label})\n",
    "                agent.reward(reward)\n",
    "                tot_rew+=reward[0]\n",
    "            print('episode: ',episode,'avg reward: ',tot_rew/len(train_ds))\n",
    "        agent.set_training(False)\n",
    "        print('Training Over')\n",
    "        agent.begin()\n",
    "        for sample,label in self.test_ds:\n",
    "            action=agent.act(sample)\n",
    "            reward=(self.accuracy(action,label),'default',{})\n",
    "            agent.reward(reward)\n",
    "            self.test_rewL+=[reward]\n",
    "            self.test_rew+=reward[0]\n",
    "        print('Test Over; Accuracy: ',self.test_rew/len(self.test_ds))\n",
    "        return self.test_rew/len(self.test_ds)\n",
    "    def accuracy(self,action,label):\n",
    "        if action==label: return 1\n",
    "        else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95733e-f863-432b-ac20-753d221e6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAgent(AIAgent):\n",
    "    def __init__(self,action_space,net):\n",
    "        super().__init__()\n",
    "        ##Augmenting AIAgent\n",
    "        self.actor=self.Actor(parent=self,model=net)\n",
    "        self.action_space=action_space\n",
    "        self.tot_rew=0\n",
    "        self.rewL=[]\n",
    "        \n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent,model):\n",
    "            super().__init__(parent=parent,model=model)\n",
    "        def call_model(self,state):\n",
    "            ##Overriding AIAgent\n",
    "            lpreds=self.model(state)\n",
    "            action=torch.argmax(lpreds,axis=1)\n",
    "            return action\n",
    "\n",
    "    def set_training(self,value):\n",
    "        self.training=value\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)\n",
    "    def reward(self,rew):\n",
    "        ##Augmenting AIAgent\n",
    "        if self.training:\n",
    "            prev_state=self.memory.sar_memory[self.time-1]['state']\n",
    "            net=self.actor.model\n",
    "            action=torch.argmax(net(prev_state))\n",
    "            prev_action=self.memory.sar_memory[self.time-1]['action']\n",
    "            net,_,_=models.Train(net,[(prev_state,rew[2]['label'])],epochs=1)\n",
    "        self.tot_rew+=rew[0]\n",
    "        return super().reward(rew)\n",
    "    def begin(self):\n",
    "        ##Augmenting AIAgent\n",
    "        self.rewL+=[self.tot_rew]\n",
    "        super().begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48ae6d-73d0-40db-b4ef-2bf1de69d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, dloader = utils.euclideanDataset(n_samples=10000,n_features=20,n_classes=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba7a9b-b0d8-4b2d-985c-fa292e13fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=[(s.unsqueeze(0),l.unsqueeze(0)) for s,l in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227e2aa-d30f-4136-a3ac-042c2684a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds=[(s.unsqueeze(0),l.unsqueeze(0)) for s,l in test_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eafa0f4-1fe2-4d6f-960f-7a0b5c2d4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=models.MLP(dims=[20,32,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed6686-899a-4f14-9114-fc6f42271701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net,_,_=models.Train(net,train_ds,epochs=5,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e88101-ba10-43b7-a1b2-85496eae3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slworld=SLWorld(train_ds,test_ds,n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fa5ee-4969-4861-9c22-648e46005c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=MLPAgent(slworld.action_space,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80584f7-7b2a-4d30-9c7d-a44213db71ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slworld.run(agent=agent,n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e24475-66fc-4544-a0fe-20297b9246ac",
   "metadata": {},
   "source": [
    "### Training an AI Agent's Model using an off-the shelf RL procedure\n",
    "Create a local environment called by a Monitor thread running within Agent, which implements or\n",
    "re-uses an on-policy RL training procedure (such as PPO etc.). The env has an input and output queue. If a training flag is set, the Agent uses a ProxyModel place of its normal model to compute actions: The actor-state is placed in the env's input queue and an action is awaited from the env's output queue.\n",
    "\n",
    "The monitor starts by calling the env.reset method that waits on the input queue to receive \n",
    "and then return a state. The monitor thread computes an action on the current state and calls\n",
    "env.step(action), which places the action in the output queue and awaits a reward from the input\n",
    "queue. After receiving a reward, step again waits for the next state on the input queue.\n",
    "Once this is also received, both next stte and reward are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefe229-241d-42fb-b5a2-0fe4b4f36c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819bbb0-dc30-46d0-9c8e-3c5cf11e74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(RandomAIAgent):\n",
    "    def __init__(self,action_space,observation_space,training_steps=20000):\n",
    "        ##Augmenting AIAgent\n",
    "        # self.model=model = PPO.load('ReinforcementLearningCourse-main/Training/Saved Models/PPO_model', env=env)\n",
    "        super().__init__(action_space)\n",
    "        self.env=self.TrainingEnv(parent=self)\n",
    "        self.env.observation_space=observation_space\n",
    "        self.model=PPO('MlpPolicy', self.env, verbose=0)\n",
    "        self.monitor=self.Monitor(parent=self)\n",
    "        # self.monitorthread=Thread(target=self.monitor.run) #For dubugging\n",
    "        self.set_training(True)\n",
    "        self.monitorthread=Thread(target=self.monitor.train,args=(training_steps,))\n",
    "        self.monitorthread.start()\n",
    "        self.tot_rew=0\n",
    "        self.logL=[]\n",
    "    \n",
    "    def log(self,entry):\n",
    "        self.logL+=[entry]\n",
    "        \n",
    "    def set_training(self,value):\n",
    "        self.training=value\n",
    "        \n",
    "    class TrainingEnv(gym.Env):\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "            self.action_space=spaces.Discrete(2)\n",
    "            # self.observation_space=spaces.Box(\n",
    "            #     low=np.array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38]), \n",
    "            #     high=np.array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38]), \n",
    "            #     shape=(4,), dtype=np.float32)\n",
    "            self.inputS=Queue() #written by act read by reset and step\n",
    "            self.outputS=Queue() #written by act read by act\n",
    "            self.rewardI=Queue() #written by act and read by step\n",
    "            self.actionO=Queue() #written by step and read by act\n",
    "            self.counter=0\n",
    "        def reset(self):\n",
    "            # print('reset')\n",
    "            state=self.inputS.get()\n",
    "            return state\n",
    "        def step(self,action):\n",
    "            # print('step')\n",
    "            self.actionO.put(action)\n",
    "            reward,done,info=self.rewardI.get()\n",
    "            next_state=self.inputS.get()\n",
    "            self.counter+=1\n",
    "            # print(self.counter,done)\n",
    "            return next_state,reward,done,info\n",
    "        def print_queues(self):\n",
    "            print('inputS',self.inputS.queue)\n",
    "            print('actionO',self.actionO.queue)\n",
    "            print('rewardI',self.rewardI.queue)\n",
    "            print('outputS',self.outputS.queue)\n",
    "             \n",
    "    class Monitor():\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "        def run(self):\n",
    "            state=env.reset()\n",
    "            for episode in range(600):\n",
    "                # env.render()\n",
    "                action=self.parent.env.action_space.sample()\n",
    "                next_state, reward, done, info = self.parent.env.step(action)\n",
    "                print(next_state, reward, done, info, action)\n",
    "            self.parent.monitorthread.join()\n",
    "        def train(self,training_steps):\n",
    "            global world_over\n",
    "            self.parent.model.learn(total_timesteps=training_steps)\n",
    "            print('Training Over')\n",
    "            self.parent.set_training(False)\n",
    "            self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "    def call_model(self,state):\n",
    "        ##Overriding AIAgent\n",
    "        if self.training:\n",
    "            self.env.inputS.put(state)\n",
    "            try: action = self.env.actionO.get(timeout=5)\n",
    "            except: action=0\n",
    "        else: action, _states = self.model.predict(state)\n",
    "        return action\n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        reward=super().reward(reward)\n",
    "        if self.training: self.env.rewardI.put(reward)\n",
    "        # super().reward(reward)\n",
    "    def begin(self,state):\n",
    "        ##Augmenting AIAgent\n",
    "        if self.training: self.env.inputS.put(state)\n",
    "        super().begin(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f938dfc-36bd-4e18-87c0-584c3c8c0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=PPOAgent(env.action_space,env.observation_space,training_steps=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edef18-00b7-4098-96f8-01f51bccc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.debug=False\n",
    "agent.use_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cce67f-0995-42fa-b1a1-1753133a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rewL=[]\n",
    "agent.tot_rew=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7d8e-eae8-43f4-98b2-28f652813c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=CartWorld(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de878c07-e022-4e18-96dc-52c98494bf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world.run(agent,n_episodes=2000,episode_maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4a528-95bd-46df-89b5-24208287a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24174f7-ddec-495e-bd52-9612889571c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.gradient(agent.rewL).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b52fb-577e-4ebd-9b8d-d63b3c1680fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(agent.rewL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b97138-511d-4bac-acac-b20f19e2c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread in threading.enumerate(): \n",
    "    print(thread.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
